{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Deploy <font color='#6957FF'>Widn Vesuvius</font> Model Package from AWS Marketplace \n"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Widn Vesuvius** is a multilingual LLM based on Unbabel's powerful Tower LLMs, optimized for high-quality translation use cases across multiple domains. It is the smallest and fastest model offered by Widn.\n",
    "\n",
    "This sample notebook shows you how to deploy <font color='#C9FF33'>[Widn Vesuvius](https://aws.amazon.com/marketplace/pp/prodview-p6qef5uamaqb6)</font> using Amazon SageMaker.\n",
    "\n",
    "> **Note**: This model package only supports SageMaker Realtime Inference. Sagemaker doesn't yet support Batch Transform for this type of model package; this is a limitation of the SageMaker Batch Transform service.\n",
    "\n",
    "\n",
    "## Pre-requisites:\n",
    "1. Ensure that IAM role used has **AmazonSageMakerFullAccess**\n",
    "1. To deploy this ML model successfully, ensure that:\n",
    "    1. Either your IAM role has these three permissions and you have authority to make AWS Marketplace subscriptions in the AWS account used: \n",
    "        1. **aws-marketplace:ViewSubscriptions**\n",
    "        1. **aws-marketplace:Unsubscribe**\n",
    "        1. **aws-marketplace:Subscribe**  \n",
    "    2. or your AWS account has a subscription to <font color='#C9FF33'>[Widn Vesuvius](https://aws.amazon.com/marketplace/pp/prodview-p6qef5uamaqb6)</font>. If so, skip step [Subscribe to the model package](#1.-Subscribe-to-the-model-package)\n",
    "\n",
    "## Contents:\n",
    "1. [Subscribe to the model package](#1.-Subscribe-to-the-model-package)\n",
    "2. [Create an endpoint and perform real-time inference](#2.-Create-an-endpoint-and-perform-real-time-inference)\n",
    "   1. [Create an endpoint](#A.-Create-an-endpoint)\n",
    "   2. [Create input payload](#B.-Create-input-payload)\n",
    "   3. [Perform real-time inference](#C.-Perform-real-time-inference)\n",
    "   4. [Visualize output](#D.-Visualize-output)\n",
    "   5. [Delete the endpoint](#E.-Streaming-output)\n",
    "3. [Example use cases](#3.-Example-use-cases) \n",
    "4. [Clean-up](#4.-Clean-up)\n",
    "    1. [Delete the model](#A.-Delete-endpoint-and-model)\n",
    "    2. [Unsubscribe to the listing (optional)](#B.-Unsubscribe-to-the-listing-(optional))\n",
    "    \n",
    "\n",
    "## Usage instructions\n",
    "You can run this notebook one cell at a time (by using Shift+Enter for running a cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Subscribe to the model package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To subscribe to the model package:\n",
    "1. Open the model package listing page <font color='#C9FF33'>[Widn Vesuvius](https://aws.amazon.com/marketplace/pp/prodview-p6qef5uamaqb6)</font> in AWS Marketplace.\n",
    "1. On the AWS Marketplace listing, click on the **Continue to subscribe** button.\n",
    "1. On the **Subscribe to this software** page, review and click on **\"Accept Offer\"** if you and your organization agrees with EULA, pricing, and support terms. \n",
    "1. Once you click on **Continue to configuration button** and then choose a **region**, you will see a **Product Arn** displayed. This is the model package ARN that you need to specify while creating a deployable model using Boto3. Copy the ARN corresponding to your region and specify the same in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T14:50:18.165738Z",
     "start_time": "2024-11-08T14:50:18.160554Z"
    }
   },
   "source": [
    "# Set the region to which you subscribed to the model package\n",
    "region = \"us-east-1\"\n",
    "\n",
    "model_package_arn = f\"arn:aws:sagemaker:{region}:571600842260:model-package/widn-tower-3-0-vesuvius-241001\""
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to load environment variables from a .env file, you can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install python-dotenv\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import json\n",
    "\n",
    "from sagemaker import ModelPackage, Predictor, Session\n",
    "from sagemaker import deserializers, get_execution_role, serializers"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "session = Session()\n",
    "region = session.boto_region_name\n",
    "role = get_execution_role(session)  # Or directly specify a suitable role\n",
    "role"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T14:51:01.333078Z",
     "start_time": "2024-11-08T14:51:01.330934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Choose an instance type to run the model\n",
    "instance_type = \"ml.g5.48xlarge\""
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T14:51:01.576890Z",
     "start_time": "2024-11-08T14:51:01.574373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = model_package_arn.split(\"/\")[-1]\n",
    "\n",
    "# Create a deployable model from the model package\n",
    "model = ModelPackage(\n",
    "    role=role,\n",
    "    model_package_arn=model_package_arn,\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "print(f\"{model_name=}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name='widn-tower-3-0-vesuvius-241001'\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Create an endpoint and perform real-time inference"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "If you want to understand how real-time inference with Amazon SageMaker works, [see their documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-hosting.html)."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### A. Create an endpoint"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T14:58:39.390661Z",
     "start_time": "2024-11-08T14:51:03.255125Z"
    }
   },
   "source": [
    "# Deploy the model\n",
    "model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=model_name,\n",
    "    container_startup_health_check_timeout=500,  # Change if necessary; about 8 minutes is usually enough\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------!"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once endpoint has been created, you should be able to perform real-time inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Create input payload"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T14:39:59.750596Z",
     "start_time": "2024-11-07T14:39:59.744552Z"
    }
   },
   "source": [
    "# Load the input samples from a jsonlines file in data/input/real-time/source_sentences.jsonl\n",
    "file_name = \"data/input/real-time/source_sentences.jsonl\"\n",
    "\n",
    "with open(file_name, \"r\") as f:\n",
    "    inputs = [json.loads(line) for line in f]\n",
    "inputs"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'source_text': 'A group of researchers has launched a new model for translation-related tasks.',\n",
       "  'source_language': 'English',\n",
       "  'target_language': 'Spanish'},\n",
       " {'source_text': 'The new model is based on a transformer architecture.',\n",
       "  'source_language': 'English',\n",
       "  'target_language': 'French'},\n",
       " {'source_text': 'The researchers have achieved state-of-the-art results on several benchmarks.',\n",
       "  'source_language': 'English',\n",
       "  'target_language': 'German'},\n",
       " {'source_text': 'Um grupo de investigadores lançou um novo modelo para tarefas relacionadas com tradução.',\n",
       "  'source_language': 'Portuguese',\n",
       "  'target_language': 'English'},\n",
       " {'source_text': 'O novo modelo é baseado numa arquitetura de transformador.',\n",
       "  'source_language': 'Portuguese',\n",
       "  'target_language': 'Italian'},\n",
       " {'source_text': 'Os investigadores alcançaram resultados de ponta em vários benchmarks.',\n",
       "  'source_language': 'Portuguese',\n",
       "  'target_language': 'Dutch'},\n",
       " {'source_text': 'Un groupe de chercheurs a lancé un nouveau modèle pour des tâches liées à la traduction.',\n",
       "  'source_language': 'French',\n",
       "  'target_language': 'Spanish'},\n",
       " {'source_text': 'Le nouveau modèle est basé sur une architecture de transformateur.',\n",
       "  'source_language': 'French',\n",
       "  'target_language': 'Portuguese'},\n",
       " {'source_text': 'Les chercheurs ont obtenu des résultats de pointe sur plusieurs benchmarks.',\n",
       "  'source_language': 'French',\n",
       "  'target_language': 'Japanese'},\n",
       " {'source_text': 'Un grupo de investigadores ha lanzado un nuevo modelo para tareas relacionadas con la traducción.',\n",
       "  'source_language': 'Spanish',\n",
       "  'target_language': 'English'},\n",
       " {'source_text': 'El nuevo modelo se basa en una arquitectura de transformador.',\n",
       "  'source_language': 'Spanish',\n",
       "  'target_language': 'Chinese'},\n",
       " {'source_text': 'Los investigadores han logrado resultados de vanguardia en varios benchmarks.',\n",
       "  'source_language': 'Spanish',\n",
       "  'target_language': 'Russian'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T14:40:00.953564Z",
     "start_time": "2024-11-07T14:40:00.951264Z"
    }
   },
   "source": [
    "# Use the following template to create a suitable prompt for translation\n",
    "prompt_template = (\n",
    "    \"\"\"\n",
    "    Translate the following text from {source_language} into {target_language}.\n",
    "    {source_language}: {source_text}\n",
    "    {target_language}:\n",
    "    \"\"\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Perform real-time inference"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T14:59:14.094732Z",
     "start_time": "2024-11-08T14:59:14.090602Z"
    }
   },
   "source": [
    "predictor = Predictor(\n",
    "    endpoint_name=model_name,\n",
    "    sagemaker_session=session,\n",
    "    serializer=serializers.JSONSerializer(),\n",
    "    deserializer=deserializers.JSONDeserializer(),\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T14:40:06.478726Z",
     "start_time": "2024-11-07T14:40:06.475474Z"
    }
   },
   "source": [
    "outputs = (\n",
    "    predictor.predict(\n",
    "        {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": prompt_template.format(**sample)},\n",
    "            ],\n",
    "            \"max_tokens\": 256,\n",
    "            \"temperature\": 0.0,\n",
    "            # Check available parameters here: https://docs.djl.ai/master/docs/serving/serving/docs/lmi/user_guides/chat_input_output_schema.html#request-schema\n",
    "        }\n",
    "    )\n",
    "    for sample in inputs\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Visualize output"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T14:40:14.482892Z",
     "start_time": "2024-11-07T14:40:08.639613Z"
    }
   },
   "source": [
    "for output in outputs:\n",
    "    # output is a dictionary with the final response\n",
    "    print(output[\"choices\"][0][\"message\"][\"content\"])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Un grupo de investigadores ha lanzado un nuevo modelo para tareas relacionadas con la traducción.\n",
      "Le nouveau modèle est basé sur une architecture de transformateur.\n",
      "Die Forscher haben auf mehreren Benchmarks hochmoderne Ergebnisse erzielt.\n",
      "A group of researchers has launched a new model for translation-related tasks.\n",
      "Il nuovo modello si basa su un'architettura di trasformatore.\n",
      "De onderzoekers behaalden baanbrekende resultaten op verschillende benchmarks.\n",
      "Un grupo de investigadores ha lanzado un nuevo modelo para tareas relacionadas con la traducción.\n",
      "O novo modelo é baseado numa arquitetura de transformador.\n",
      "研究者は、複数のベンチマークで最先端の結果を得た。\n",
      "A group of researchers has launched a new model for translation-related tasks.\n",
      "新模型基于变压器架构。\n",
      "Исследователи добились передовых результатов по нескольким эталонам.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### E. Streaming output"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T14:40:23.908939Z",
     "start_time": "2024-11-07T14:40:19.075580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Call the model and handle the streaming response\n",
    "# There are three important differences to note:\n",
    "# 1. The method is `predict_stream` and the \"stream\" parameter is set to True;\n",
    "# 2. The response is a generator that yields partial JSON objects;\n",
    "# 3. The response schema is different from the non-streaming case: use the `delta` field instead of `message`.\n",
    "\n",
    "outputs = (\n",
    "    predictor.predict_stream(\n",
    "        {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": prompt_template.format(**sample)},\n",
    "            ],\n",
    "            \"max_tokens\": 256,\n",
    "            \"temperature\": 0.0,\n",
    "            \"stream\": True,\n",
    "        }\n",
    "    )\n",
    "    for sample in inputs\n",
    ")\n",
    "\n",
    "for output in outputs:\n",
    "    bytes_accumulated = bytearray()\n",
    "    for payload in output:\n",
    "        bytes_accumulated.extend(payload)\n",
    "        try:\n",
    "            item = json.loads(bytes_accumulated)\n",
    "        except json.decoder.JSONDecodeError:\n",
    "            # payload contained only a partial JSON object\n",
    "            continue\n",
    "    \n",
    "        bytes_accumulated = bytearray()\n",
    "        if item is not None:\n",
    "            print(item[\"choices\"][0][\"delta\"][\"content\"], end=\"\")\n",
    "    print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Un grupo de investigadores ha lanzado un nuevo modelo para tareas relacionadas con la traducción.\n",
      "Le nouveau modèle est basé sur une architecture de transformateur.\n",
      "Die Forscher haben auf mehreren Benchmarks hochmoderne Ergebnisse erzielt.\n",
      "A group of researchers has launched a new model for translation-related tasks.\n",
      "Il nuovo modello si basa su un'architettura di trasformatore.\n",
      "De onderzoekers behaalden baanbrekende resultaten op verschillende benchmarks.\n",
      "Un grupo de investigadores ha lanzado un nuevo modelo para tareas relacionadas con la traducción.\n",
      "O novo modelo é baseado numa arquitetura de transformador.\n",
      "研究者は、複数のベンチマークで最先端の結果を得た。\n",
      "A group of researchers has launched a new model for translation-related tasks.\n",
      "新模型基于变压器架构。\n",
      "Исследователи добились передовых результатов по нескольким эталонам.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Example use cases\n",
    "\n",
    "You can experiment with your own prompts but in this section we show some examples on how to use it for some more complex translation tasks.\n",
    "\n",
    "### Document-level translation\n",
    "\n",
    "The model is trained with a context length of 4096 tokens and thus we recommend translation of documents that have less than 4k tokens (which are roughly 14k characters).\n",
    "\n",
    "Example prompt:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T15:08:05.216524Z",
     "start_time": "2024-11-08T15:08:05.210401Z"
    }
   },
   "source": [
    "prompt = \"\"\"\n",
    "You are an expert translation large language model tasked with translating texts from English to German.\n",
    "\n",
    "Translate the following text into German:\n",
    "### Early Life and Career Beginnings\n",
    "Born on February 5, 1985, in Madeira, Portugal, Cristiano Ronaldo began his football journey at Sporting CP before catching the eye of major European clubs.\n",
    "\n",
    "### Professional Career\n",
    "- **Manchester United** (2003-2009, 2021-2022): Rose to fame, winning multiple Premier League titles and the 2008 Champions League.\n",
    "- **Real Madrid** (2009-2018): Cemented his legacy, becoming the club’s all-time top scorer and winning four Champions League titles.\n",
    "- **Juventus** (2018-2021): Continued success in Italy, securing two Serie A titles.\n",
    "- **Al-Nassr** (2023-Present): Currently playing in Saudi Arabia, continuing to break records.\n",
    "\n",
    "### Playing Style\n",
    "Renowned for his versatility, speed, and goal-scoring prowess, Ronaldo is known for his athleticism and precision both in the air and on the ground.\n",
    "\n",
    "### Achievements\n",
    "- **Ballon d'Or**: 5 wins\n",
    "- **Champions League Titles**: 5\n",
    "- **UEFA Euro Champion**: 2016 with Portugal\n",
    "\n",
    "### Personal Life and Influence\n",
    "Beyond football, Ronaldo is a global icon with massive influence, known for his philanthropy and endorsements, making him one of the world’s most marketable athletes.\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T15:08:13.257976Z",
     "start_time": "2024-11-08T15:08:07.108617Z"
    }
   },
   "source": [
    "# Call the model and show the output\n",
    "output = predictor.predict(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        \"max_tokens\": 1000,\n",
    "        \"temperature\": 0.0,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(output[\"choices\"][0][\"message\"][\"content\"])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Frühes Leben und Karrierebeginn\n",
      "Geboren am 5. Februar 1985 in Madeira, Portugal, begann Cristiano Ronaldo seine Fußballkarriere bei Sporting CP, bevor er die Aufmerksamkeit großer europäischer Vereine erregte.\n",
      "\n",
      "### Profikarriere\n",
      "- **Manchester United** (2003-2009, 2021-2022): Erlangte Ruhm und gewann mehrere Premier League-Titel sowie die Champions League 2008.\n",
      "- **Real Madrid** (2009-2018): Festigte sein Vermächtnis und wurde der erfolgreichste Spieler aller Zeiten des Vereins, indem er vier Champions League-Titel gewann.\n",
      "- **Juventus** (2018-2021): Erhielt in Italien weiterhin Erfolg und sicherte sich zwei Serie A-Titel.\n",
      "- **Al-Nassr** (2023-Gegenwärtig): Spielt derzeit in Saudi-Arabien und setzt weiterhin Rekorde.\n",
      "\n",
      "### Spielstil\n",
      "Ronaldo ist bekannt für seine Vielseitigkeit, Schnelligkeit und Torwartfertigkeit und ist für seine Athletik und Präzision sowohl in der Luft als auch auf dem Boden bekannt.\n",
      "\n",
      "### Erfolge\n",
      "- **Ballon d'Or**: 5 Siege\n",
      "- **Champions League-Titel**: 5\n",
      "- **UEFA Euro Champion**: 2016 mit Portugal\n",
      "\n",
      "### Persönliches Leben und Einfluss\n",
      "Über den Fußball hinaus ist Ronaldo ein globales Symbol mit großem Einfluss, bekannt für seine Philanthropie und Sponsoring-Vereinbarungen, was ihn zu einem der weltweit umsatzstärksten Athleten macht.\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glossaries, tone, and more\n",
    "\n",
    "You can give the model a couple more details from the expected translation. Things like tone, glossaries and few-shot examples. You can also pass those individually but to simplify, in this example, we mix all together.\n",
    "\n",
    "Example prompt:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T14:40:36.751519Z",
     "start_time": "2024-11-07T14:40:36.748964Z"
    }
   },
   "source": [
    "prompt = \"\"\"\n",
    "Translation Task:\n",
    "- From: English\n",
    "- To: Portuguese (Portugal)\n",
    "- Style: formal\n",
    "Reference Translations:\n",
    "Example 1:\n",
    "  English: To keep your trip on track, we've got you a place to stay:\n",
    "  Portuguese (Portugal): Para que possa continuar a sua viagem, encontrámos um local para a sua estadia:\n",
    "Example 2:\n",
    "  English: To keep your trip on track, we've found you a similar place to stay:\n",
    "  Portuguese (Portugal): Para que possa continuar a sua viagem, encontrámos um local semelhante para a sua estadia:\n",
    "Example 3:\n",
    "  English: To keep your trip on track, we've found you a similar place to stay:\n",
    "  Portuguese (Portugal): Para o ajudarmos a reorganizar a sua viagem, encontrámos um alojamento semelhante para si:\n",
    "Example 4:\n",
    "  English: To keep your trip on track, we've found you a similar place to stay:\n",
    "  Portuguese (Portugal): Para poder continuar com a sua viagem, encontrámos um local semelhante para a sua estadia:\n",
    "Example 5:\n",
    "  English: To keep your trip on track, we've found you a similar place to stay:\n",
    "  Portuguese (Portugal): Para poder continuar com a sua viagem, encontrámos um local semelhante onde poderá ficar hospedado:\n",
    "\n",
    "Text to Translate:\n",
    "To keep your trip on track, we've found you a alternative place to stay:\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T14:40:38.111019Z",
     "start_time": "2024-11-07T14:40:37.522553Z"
    }
   },
   "source": [
    "# Call the model and show the output\n",
    "output = predictor.predict(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        \"max_tokens\": 256,\n",
    "        \"temperature\": 0.0,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(output[\"choices\"][0][\"message\"][\"content\"])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para que possa continuar a sua viagem, encontrámos um local alternativo para a sua estadia:\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-context translation\n",
    "\n",
    "Sometimes, when documents are too large, we might need to break translations into paragraphs and perform `incontext translations`.\n",
    "\n",
    "Example prompt:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T14:40:40.140282Z",
     "start_time": "2024-11-07T14:40:40.137934Z"
    }
   },
   "source": [
    "prompt = \"\"\"\n",
    "Consider the following source side context:\n",
    "\n",
    "A giant Buddha Amitabha statue, 27 metres tall and weighing 3,000 tonnes, was installed outside on the Lan Kha Mountain in 2010. It was adapted from a similar structure from the Ly Dynasty.\n",
    "The Phat Tich Pagoda is associated with Tu Thuc’s meeting with a fairy. As the legend goes, there were endless peonies on Lan Kha Mountain and in the pagoda, leading a young woman to visit the pagoda one day to see the flowers. She carelessly broke a tree branch and was fined by the monks, but a local scholar, Tu Thuc, was also visiting the pagoda and offered his coat to compensate for the broken branch. They became friends and continued to meet at the pagoda. The woman ultimately invited Tu Thuc to visit her house, leading him to a peony forest and into a cave on the mountainside with an imperial palace with high walls and stone footsteps. She revealed that she was a fairy and they got married.\n",
    "Every year, people visit the pagoda to take part in the peony festival, where they enjoy looking at the flowers, listening to quan ho (love duets) and poem recitations, and playing traditional games. The festival usually lasts two days.\n",
    "The pagoda was recognised as a national relic site in 1962 and a special national relic site in 2014.\n",
    "\n",
    "Using the source context, translate the sentence below into Chinese (Simplified):\n",
    "The Phat Tich pagoda, a special national relic site located just 25 kilometres northeast of Hanoi, was built in 1057 on a mountain called Lan Kha during the reign of King Ly Thanh Tong (1054-72). It was reduced to ashes by French colonialists in 1948 and restored in 1987.\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T14:40:42.823445Z",
     "start_time": "2024-11-07T14:40:41.320341Z"
    }
   },
   "source": [
    "# Call the model and show the output\n",
    "output = predictor.predict(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        \"max_tokens\": 256,\n",
    "        \"temperature\": 0.0,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(output[\"choices\"][0][\"message\"][\"content\"])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "法提寺塔位于河内东北25公里处，是国家级文物保护单位，建于1057年，位于兰卡山，当时是黎朝统治时期（1054-72年）。1948年被法国殖民者烧毁，1987年修复。\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Delete endpoint and model\n",
    "\n",
    "Once you are done using the endpoint, to avoid unnecessary costs, you can delete it and the model by running the following commands:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T15:09:29.159495Z",
     "start_time": "2024-11-08T15:09:28.167520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.sagemaker_session.delete_endpoint(model_name)\n",
    "model.sagemaker_session.delete_endpoint_config(model_name)"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### B. Unsubscribe to the listing (optional)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to unsubscribe to the model package, follow these steps. Before you cancel the subscription, ensure that you do not have any [deployable model](https://console.aws.amazon.com/sagemaker/home#/models) created from the model package. Note - You can find this information by looking at the container name associated with the model. \n",
    "\n",
    "**Steps to unsubscribe to product from AWS Marketplace**:\n",
    "1. Navigate to __Machine Learning__ tab on [__Your Software subscriptions page__](https://aws.amazon.com/marketplace/ai/library?productType=ml&ref_=mlmp_gitdemo_indust)\n",
    "2. Locate the listing that you want to cancel the subscription for, and then choose __Cancel Subscription__  to cancel the subscription.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
