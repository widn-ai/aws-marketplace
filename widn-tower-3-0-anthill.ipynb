{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy <font color='#6957FF'>Widn Tower Anthill</font> Model Package from AWS Marketplace \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "**Widn Tower Anthill** is a multilingual LLM based on Unbabel's powerful Tower LLMs, optimized for high-quality translation use cases across multiple domains. It is the smallest and fastest model offered by Widn.\n",
    "\n",
    "This sample notebook shows you how to deploy <font color='#C9FF33'>[Widn Tower Anthill](https://aws.amazon.com/marketplace/pp/prodview-xskn6yectpscq)</font> using Amazon SageMaker.\n",
    "\n",
    "> **Note**: This model package only supports SageMaker Realtime Inference. Sagemaker doesn't yet support Batch Transform for this type of model package; this is a limitation of the SageMaker Batch Transform service.\n",
    "\n",
    "\n",
    "## Pre-requisites:\n",
    "1. Ensure that IAM role used has **AmazonSageMakerFullAccess**\n",
    "1. To deploy this ML model successfully, ensure that:\n",
    "    1. Either your IAM role has these three permissions and you have authority to make AWS Marketplace subscriptions in the AWS account used: \n",
    "        1. **aws-marketplace:ViewSubscriptions**\n",
    "        1. **aws-marketplace:Unsubscribe**\n",
    "        1. **aws-marketplace:Subscribe**  \n",
    "    2. or your AWS account has a subscription to <font color='#C9FF33'>[Widn Tower Anthill](https://aws.amazon.com/marketplace/pp/prodview-xskn6yectpscq)</font>. If so, skip step [Subscribe to the model package](#1.-Subscribe-to-the-model-package)\n",
    "\n",
    "## Contents:\n",
    "1. [Subscribe to the model package](#1.-Subscribe-to-the-model-package)\n",
    "2. [Create an endpoint and perform real-time inference](#2.-Create-an-endpoint-and-perform-real-time-inference)\n",
    "   1. [Create an endpoint](#A.-Create-an-endpoint)\n",
    "   2. [Create input payload](#B.-Create-input-payload)\n",
    "   3. [Perform real-time inference](#C.-Perform-real-time-inference)\n",
    "   4. [Visualize output](#D.-Visualize-output)\n",
    "   5. [Streaming output](#E.-Streaming-output)\n",
    "3. [Example use cases](#3.-Example-use-cases) \n",
    "4. [Clean-up](#4.-Clean-up)\n",
    "    1. [Delete the model](#A.-Delete-endpoint-and-model)\n",
    "    2. [Unsubscribe to the listing (optional)](#B.-Unsubscribe-to-the-listing-(optional))\n",
    "    \n",
    "\n",
    "## Usage instructions\n",
    "You can run this notebook one cell at a time (by using Shift+Enter for running a cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Subscribe to the model package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To subscribe to the model package:\n",
    "1. Open the model package listing page <font color='#C9FF33'>[Widn Tower Anthill](https://aws.amazon.com/marketplace/pp/prodview-xskn6yectpscq)</font> in AWS Marketplace.\n",
    "1. On the AWS Marketplace listing, click on the **Continue to subscribe** button.\n",
    "1. On the **Subscribe to this software** page, review and click on **\"Accept Offer\"** if you and your organization agrees with EULA, pricing, and support terms. \n",
    "1. Once you click on **Continue to configuration button** and then choose a **region**, you will see a **Product Arn** displayed. This is the model package ARN that you need to specify while creating a deployable model using Boto3. Copy the ARN corresponding to your region and specify the same in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T11:22:46.414151Z",
     "start_time": "2024-11-15T11:22:46.410479Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set the region to which you subscribed to the model package\n",
    "region = \"us-east-1\"\n",
    "\n",
    "model_package_arn = f\"arn:aws:sagemaker:{region}:571600842260:model-package/widn-tower-3-0-anthill-241001\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to load environment variables from a .env file, you can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-dotenv\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from sagemaker import ModelPackage, Predictor, Session\n",
    "from sagemaker import deserializers, get_execution_role, serializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session()\n",
    "region = session.boto_region_name\n",
    "role = get_execution_role(session)  # Or directly specify a suitable role\n",
    "role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an instance type to run the model\n",
    "instance_type = \"ml.g5.xlarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_package_arn.split(\"/\")[-1]\n",
    "\n",
    "# Create a deployable model from the model package\n",
    "model = ModelPackage(\n",
    "    role=role,\n",
    "    model_package_arn=model_package_arn,\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "print(f\"{model_name=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create an endpoint and perform real-time inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to understand how real-time inference with Amazon SageMaker works, [see their documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-hosting.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Create an endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model\n",
    "model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=model_name,\n",
    "    container_startup_health_check_timeout=500,  # Change if necessary; about 8 minutes is usually enough\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once endpoint has been created, you should be able to perform real-time inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Create input payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the input samples from a jsonlines file in data/input/real-time/source_sentences.jsonl\n",
    "file_name = \"data/input/real-time/source_sentences.jsonl\"\n",
    "\n",
    "with open(file_name, \"r\") as f:\n",
    "    inputs = [json.loads(line) for line in f]\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following template to create a suitable prompt for translation\n",
    "prompt_template = (\n",
    "    \"\"\"\n",
    "    Translate the following text from {source_language} into {target_language}.\n",
    "    {source_language}: {source_text}\n",
    "    {target_language}:\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Perform real-time inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = Predictor(\n",
    "    endpoint_name=model_name,\n",
    "    sagemaker_session=session,\n",
    "    serializer=serializers.JSONSerializer(),\n",
    "    deserializer=deserializers.JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = (\n",
    "    predictor.predict(\n",
    "        {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": prompt_template.format(**sample)},\n",
    "            ],\n",
    "            \"max_tokens\": 256,\n",
    "            \"temperature\": 0.0,\n",
    "            # Check available parameters here: https://docs.djl.ai/master/docs/serving/serving/docs/lmi/user_guides/chat_input_output_schema.html#request-schema\n",
    "        }\n",
    "    )\n",
    "    for sample in inputs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Visualize output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for output in outputs:\n",
    "    # output is a dictionary with the final response\n",
    "    print(output[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Streaming output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the model and handle the streaming response\n",
    "# There are three important differences to note:\n",
    "# 1. The method is `predict_stream` and the \"stream\" parameter is set to True;\n",
    "# 2. The response is a generator that yields partial JSON objects;\n",
    "# 3. The response schema is different from the non-streaming case: use the `delta` field instead of `message`.\n",
    "\n",
    "outputs = (\n",
    "    predictor.predict_stream(\n",
    "        {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": prompt_template.format(**sample)},\n",
    "            ],\n",
    "            \"max_tokens\": 256,\n",
    "            \"temperature\": 0.0,\n",
    "            \"stream\": True,\n",
    "        }\n",
    "    )\n",
    "    for sample in inputs\n",
    ")\n",
    "\n",
    "for output in outputs:\n",
    "    bytes_accumulated = bytearray()\n",
    "    for payload in output:\n",
    "        bytes_accumulated.extend(payload)\n",
    "        try:\n",
    "            item = json.loads(bytes_accumulated)\n",
    "        except json.decoder.JSONDecodeError:\n",
    "            # payload contained only a partial JSON object\n",
    "            continue\n",
    "    \n",
    "        bytes_accumulated = bytearray()\n",
    "        if item is not None:\n",
    "            print(item[\"choices\"][0][\"delta\"][\"content\"], end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Example use cases\n",
    "\n",
    "You can experiment with your own prompts but in this section we show some examples on how to use it for some more complex translation tasks.\n",
    "\n",
    "### Document-level translation\n",
    "\n",
    "The model is trained with a context length of 4096 tokens and thus we recommend translation of documents that have less than 4k tokens (which are roughly 14k characters).\n",
    "\n",
    "Example prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are an expert translation large language model tasked with translating texts from English to German.\n",
    "\n",
    "Translate the following text into German:\n",
    "### Early Life and Career Beginnings\n",
    "Born on February 5, 1985, in Madeira, Portugal, Cristiano Ronaldo began his football journey at Sporting CP before catching the eye of major European clubs.\n",
    "\n",
    "### Professional Career\n",
    "- **Manchester United** (2003-2009, 2021-2022): Rose to fame, winning multiple Premier League titles and the 2008 Champions League.\n",
    "- **Real Madrid** (2009-2018): Cemented his legacy, becoming the club’s all-time top scorer and winning four Champions League titles.\n",
    "- **Juventus** (2018-2021): Continued success in Italy, securing two Serie A titles.\n",
    "- **Al-Nassr** (2023-Present): Currently playing in Saudi Arabia, continuing to break records.\n",
    "\n",
    "### Playing Style\n",
    "Renowned for his versatility, speed, and goal-scoring prowess, Ronaldo is known for his athleticism and precision both in the air and on the ground.\n",
    "\n",
    "### Achievements\n",
    "- **Ballon d'Or**: 5 wins\n",
    "- **Champions League Titles**: 5\n",
    "- **UEFA Euro Champion**: 2016 with Portugal\n",
    "\n",
    "### Personal Life and Influence\n",
    "Beyond football, Ronaldo is a global icon with massive influence, known for his philanthropy and endorsements, making him one of the world’s most marketable athletes.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the model and show the output\n",
    "output = predictor.predict(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        \"max_tokens\": 1000,\n",
    "        \"temperature\": 0.0,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(output[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glossaries, tone, and more\n",
    "\n",
    "You can give the model a couple more details from the expected translation. Things like tone, glossaries and few-shot examples. You can also pass those individually but to simplify, in this example, we mix all together.\n",
    "\n",
    "Example prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Translation Task:\n",
    "- From: English\n",
    "- To: Portuguese (Portugal)\n",
    "- Style: formal\n",
    "Reference Translations:\n",
    "Example 1:\n",
    "  English: To keep your trip on track, we've got you a place to stay:\n",
    "  Portuguese (Portugal): Para que possa continuar a sua viagem, encontrámos um local para a sua estadia:\n",
    "Example 2:\n",
    "  English: To keep your trip on track, we've found you a similar place to stay:\n",
    "  Portuguese (Portugal): Para que possa continuar a sua viagem, encontrámos um local semelhante para a sua estadia:\n",
    "Example 3:\n",
    "  English: To keep your trip on track, we've found you a similar place to stay:\n",
    "  Portuguese (Portugal): Para o ajudarmos a reorganizar a sua viagem, encontrámos um alojamento semelhante para si:\n",
    "Example 4:\n",
    "  English: To keep your trip on track, we've found you a similar place to stay:\n",
    "  Portuguese (Portugal): Para poder continuar com a sua viagem, encontrámos um local semelhante para a sua estadia:\n",
    "Example 5:\n",
    "  English: To keep your trip on track, we've found you a similar place to stay:\n",
    "  Portuguese (Portugal): Para poder continuar com a sua viagem, encontrámos um local semelhante onde poderá ficar hospedado:\n",
    "\n",
    "Text to Translate:\n",
    "To keep your trip on track, we've found you a alternative place to stay:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the model and show the output\n",
    "output = predictor.predict(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        \"max_tokens\": 256,\n",
    "        \"temperature\": 0.0,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(output[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-context translation\n",
    "\n",
    "Sometimes, when documents are too large, we might need to break translations into paragraphs and perform `incontext translations`.\n",
    "\n",
    "Example prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Consider the following source side context:\n",
    "\n",
    "A giant Buddha Amitabha statue, 27 metres tall and weighing 3,000 tonnes, was installed outside on the Lan Kha Mountain in 2010. It was adapted from a similar structure from the Ly Dynasty.\n",
    "The Phat Tich Pagoda is associated with Tu Thuc’s meeting with a fairy. As the legend goes, there were endless peonies on Lan Kha Mountain and in the pagoda, leading a young woman to visit the pagoda one day to see the flowers. She carelessly broke a tree branch and was fined by the monks, but a local scholar, Tu Thuc, was also visiting the pagoda and offered his coat to compensate for the broken branch. They became friends and continued to meet at the pagoda. The woman ultimately invited Tu Thuc to visit her house, leading him to a peony forest and into a cave on the mountainside with an imperial palace with high walls and stone footsteps. She revealed that she was a fairy and they got married.\n",
    "Every year, people visit the pagoda to take part in the peony festival, where they enjoy looking at the flowers, listening to quan ho (love duets) and poem recitations, and playing traditional games. The festival usually lasts two days.\n",
    "The pagoda was recognised as a national relic site in 1962 and a special national relic site in 2014.\n",
    "\n",
    "Using the source context, translate the sentence below into Chinese (Simplified):\n",
    "The Phat Tich pagoda, a special national relic site located just 25 kilometres northeast of Hanoi, was built in 1057 on a mountain called Lan Kha during the reign of King Ly Thanh Tong (1054-72). It was reduced to ashes by French colonialists in 1948 and restored in 1987.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the model and show the output\n",
    "output = predictor.predict(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        \"max_tokens\": 256,\n",
    "        \"temperature\": 0.0,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(output[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Delete endpoint and model\n",
    "\n",
    "Once you are done using the endpoint, to avoid unnecessary costs, you can delete it and the model by running the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sagemaker_session.delete_endpoint(model_name)\n",
    "model.sagemaker_session.delete_endpoint_config(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Unsubscribe to the listing (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to unsubscribe to the model package, follow these steps. Before you cancel the subscription, ensure that you do not have any [deployable model](https://console.aws.amazon.com/sagemaker/home#/models) created from the model package. Note - You can find this information by looking at the container name associated with the model. \n",
    "\n",
    "**Steps to unsubscribe to product from AWS Marketplace**:\n",
    "1. Navigate to __Machine Learning__ tab on [__Your Software subscriptions page__](https://aws.amazon.com/marketplace/ai/library?productType=ml&ref_=mlmp_gitdemo_indust)\n",
    "2. Locate the listing that you want to cancel the subscription for, and then choose __Cancel Subscription__  to cancel the subscription.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
